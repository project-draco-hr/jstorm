{
  this.conf=conf;
  this.context=context;
  this.topologyId=topology_id;
  this.supervisorId=supervisor_id;
  this.port=port;
  this.workerId=worker_id;
  this.active=new AtomicBoolean(true);
  this.topologyStatus=StatusType.active;
  if (StormConfig.cluster_mode(conf).equals("distributed")) {
    String pidDir=StormConfig.worker_pids_root(conf,worker_id);
    JStormServerUtils.createPid(pidDir);
  }
  this.zkClusterstate=ZkTool.mk_distributed_cluster_state(conf);
  this.zkCluster=Cluster.mk_storm_cluster_state(zkClusterstate);
  Map rawConf=StormConfig.read_supervisor_topology_conf(conf,topology_id);
  this.stormConf=new HashMap<Object,Object>();
  this.stormConf.putAll(conf);
  this.stormConf.putAll(rawConf);
  LOG.info("Worker Configuration " + stormConf);
  try {
    if (jar_path != null) {
      String[] paths=jar_path.split(":");
      Set<URL> urls=new HashSet<URL>();
      for (      String path : paths) {
        if (StringUtils.isBlank(path))         continue;
        URL url=new URL("File:" + path);
        urls.add(url);
      }
      WorkerClassLoader.mkInstance(urls.toArray(new URL[0]),ClassLoader.getSystemClassLoader(),ClassLoader.getSystemClassLoader().getParent(),ConfigExtension.isEnableTopologyClassLoader(stormConf));
    }
 else {
      WorkerClassLoader.mkInstance(new URL[0],ClassLoader.getSystemClassLoader(),ClassLoader.getSystemClassLoader().getParent(),ConfigExtension.isEnableTopologyClassLoader(stormConf));
    }
  }
 catch (  Exception e) {
    LOG.error("init jarClassLoader error!",e);
    throw new InvalidParameterException();
  }
  if (this.context == null) {
    this.context=TransportFactory.makeContext(stormConf);
  }
  boolean disruptorUseSleep=ConfigExtension.isDisruptorUseSleep(stormConf);
  DisruptorQueue.setUseSleep(disruptorUseSleep);
  LOG.info("Disruptor use sleep " + disruptorUseSleep);
  int buffer_size=Utils.getInt(conf.get(Config.TOPOLOGY_TRANSFER_BUFFER_SIZE));
  WaitStrategy waitStrategy=(WaitStrategy)Utils.newInstance((String)conf.get(Config.TOPOLOGY_DISRUPTOR_WAIT_STRATEGY));
  this.transferQueue=new DisruptorQueue("TotalTransfer",ProducerType.MULTI,buffer_size,waitStrategy);
  this.transferQueue.consumerStarted();
  this.sendingQueue=new DisruptorQueue("TotalSending",ProducerType.MULTI,buffer_size,waitStrategy);
  this.sendingQueue.consumerStarted();
  this.nodeportSocket=new ConcurrentHashMap<WorkerSlot,IConnection>();
  this.taskNodeport=new ConcurrentHashMap<Integer,WorkerSlot>();
  this.workerToResource=new ConcurrentSkipListSet<ResourceWorkerSlot>();
  this.innerTaskTransfer=new ConcurrentHashMap<Integer,DisruptorQueue>();
  this.deserializeQueues=new ConcurrentHashMap<Integer,DisruptorQueue>();
  Assignment assignment=zkCluster.assignment_info(topologyId,null);
  if (assignment == null) {
    String errMsg="Failed to get Assignment of " + topologyId;
    LOG.error(errMsg);
    throw new RuntimeException(errMsg);
  }
  workerToResource.addAll(assignment.getWorkers());
  this.taskids=assignment.getCurrentWorkerTasks(supervisorId,port);
  if (taskids.size() == 0) {
    throw new RuntimeException("No tasks running current workers");
  }
  LOG.info("Current worker taskList:" + taskids);
  rawTopology=StormConfig.read_supervisor_topology_code(conf,topology_id);
  sysTopology=Common.system_topology(stormConf,rawTopology);
  generateMaps();
  contextMaker=new ContextMaker(this);
  metricReporter=new MetricReporter(this);
  LOG.info("Successfully create WorkerData");
}
